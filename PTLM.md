<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [Repo](#repo)
  - [预训练语言模型](#预训练语言模型)
  - [蒸馏](#蒸馏)
  - [长文本PTLM](#长文本ptlm)
  - [tokenizer](#tokenizer)
  - [词向量](#词向量)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->



# Repo
- Transformer升级之路：2、博采众长的旋转式位置编码 https://github.com/ZhuiyiTechnology/roformer

## 预训练语言模型
- https://github.com/lonePatient/awesome-pretrained-chinese-nlp-models
  - 收集了一些高质量 中文 预训练语言模型
- https://github.com/dbiir/UER-py
- https://github.com/stanfordnlp/GloVe
- 华为预训练语言模型 https://github.com/huawei-noah/Pretrained-Language-Model
- bert相关 https://github.com/Jiakui/awesome-bert
- https://github.com/brightmart/roberta_zh
- https://github.com/sinovation/ZEN
- https://github.com/brightmart/albert_zh
- https://github.com/zihangdai/xlnet
- https://github.com/bojone/t5_in_bert4keras
- https://github.com/utterworks/fast-bert
- https://github.com/idiap/fast-transformers
- https://github.com/CLUEbenchmark/LightLM
- https://github.com/google-research/electra
- https://github.com/yizhen20133868/BERT-related-papers
- 

## 蒸馏
- https://github.com/kevinmtian/distill-bert
- https://github.com/wangbq18/distillation_model_keras_bert
- https://github.com/qiangsiwei/bert_distill
- https://github.com/PaddlePaddle/ERNIE
- https://github.com/YunwenTechnology/Unilm
- https://github.com/google-research/text-to-text-transfer-transformer
- https://github.com/TsinghuaAI/CPM-1-Generate
- https://github.com/sinovation/ZEN
- https://github.com/dbiir/UER-py
- https://github.com/ymcui/Chinese-ELECTRA
- https://github.com/zhongerqiandan/pretrained-unilm-Chinese
- https://github.com/ymcui/Chinese-BERT-wwm
- https://github.com/ZhuiyiTechnology/WoBERT
- https://github.com/NVIDIA/Megatron-LM

## 长文本PTLM
- https://github.com/SCHENLIU/longformer-chinese
- https://github.com/allenai/longformer
- https://github.com/Sleepychord/CogLTX
- 

## tokenizer
- 基于神经网络的分词模型 https://github.com/google/sentencepiece

## 词向量
- 100+的中文词向量 https://github.com/Embedding/Chinese-Word-Vectors
- 词向量相关paper，resource，dataset https://github.com/Hironsan/awesome-embedding-models
- ngrams词向量模型 https://github.com/zhezhaoa/ngram2vec
- https://github.com/facebookresearch/fastText
- 

