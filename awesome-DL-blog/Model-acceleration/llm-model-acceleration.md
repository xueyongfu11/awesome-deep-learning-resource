[TOC]



# 大模型加速

- [全栈Transformer推理优化第二季：部署长上下文模型-翻译](https://zhuanlan.zhihu.com/p/697244539)

- [大模型推理妙招—投机采样（Speculative Decoding）](https://zhuanlan.zhihu.com/p/651359908)


# RTP-LLM

- [大模型推理优化实践：KV cache复用与投机采样](https://zhuanlan.zhihu.com/p/697801604)
  - 基于阿里RTP-LLM推理引擎的应用实践
  - 流量达到同一个实例
  - 使用投机采样技术

- [大模型推理框架RTP-LLM对LoRA的支持](https://zhuanlan.zhihu.com/p/698331657)