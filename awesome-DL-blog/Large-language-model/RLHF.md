

# 基于强化学习的对齐方法

- [大模型的PPO、DPO偏好优化算法玩不起？那建议你看一下ORPO](https://zhuanlan.zhihu.com/p/688583797)
  - ORPO

- [RLHF 和 DPO：简化和增强语言模型的微调](https://mp.weixin.qq.com/s/-5nzriCsoZIL3FKZxzbONw)

- [大模型免微调解锁对话能力，RLHF没必要了！节省大量成本和时间，一作上交大校友](https://zhuanlan.zhihu.com/p/670682075)
  - URIAL, base model的免微调方法

- [BPO：灵活的 Prompt 对齐优化技术](https://zhuanlan.zhihu.com/p/667767805)
  - BPO，主要是优化prompt

- [图解大模型RLHF系列之：人人都能看懂的PPO原理与源码解读](https://mp.weixin.qq.com/s/J8c7rEmkQH4lBj1pWntv9w)

- [使用KTO进行更好、更便宜、更快速的LLM对齐](https://mp.weixin.qq.com/s/vFrcW43jhraZT8ZaDBxl7A)

- [ChatGPT 背后的“功臣”——RLHF 技术详解](https://huggingface.co/blog/zh/rlhf)

- [OPO:无需训练实现价值观实时动态对齐：上交开源价值观对齐方法，闭源与开源大模型均适用](https://mp.weixin.qq.com/s/_CB0LBQVI_2NBiX63pyYSA)

- [深挖RLHF潜力，复旦语言和视觉团队创新奖励模型优化，让大模型更对齐](https://mp.weixin.qq.com/s/BSaGLikARlvM8yitYtlA3w)

- [Secrets of RLHF II：大模型reward model的trick](https://mp.weixin.qq.com/s/G69w-Y2Jb_SgtvLcjCs_3g)

- [Secrets of RLHF I：大模型RLHF的trick](https://zhuanlan.zhihu.com/p/646385336)

- [RLCD：无需人类反馈即可对齐！田渊栋团队新作RLCD：无害型、有益性、大纲写作全面超越基线模型](https://mp.weixin.qq.com/s/sQolnpmBdCufVVR8q6GG8w)

- [RAFT：玩不起RLHF？港科大开源高效对齐算法RAFT「木筏」，GPT扩散模型都能用 - 知乎](https://zhuanlan.zhihu.com/p/623069114)
  - RAFT

- [大语言模型之RRHF](https://zhuanlan.zhihu.com/p/622198781)
  - RRHF

- [LLM RLHF 2024论文（三）RSO](https://zhuanlan.zhihu.com/p/690198669)

- [探索最优POST-TRAINING方案](https://zhuanlan.zhihu.com/p/661323551)

- [Preference Tuning LLMs with Direct Preference Optimization Methods](https://huggingface.co/blog/pref-tuning)

- [RLHF 中的SOTA奖励函数的训练](https://zhuanlan.zhihu.com/p/688636894)

- [APO｜利用GAN的思想训练RLHF中的RM](https://zhuanlan.zhihu.com/p/674776494)