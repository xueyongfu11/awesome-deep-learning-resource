
## LLM

- [ChatGPT能写长篇小说了，ETH提出RecurrentGPT实现交互式超长文本生成](https://mp.weixin.qq.com/s/UoAEowbkvSV74P5eTUrNwg)
- [RLHF，对齐了，又没完全对齐？](https://mp.weixin.qq.com/s/0Cy5o8WKyJuqWHv3gfKDNw)
- [清华ACL 2023最新长文 | WebCPM：首个联网支持中文问答开源模型](https://mp.weixin.qq.com/s/WSzJpQBxQQKdMRRrDObICQ)
  - 提出模仿人类使用搜索引擎搜索复杂问题并与搜索引擎多次交互的中文数据集
- [如何大幅度延长语言模型输入长度](https://mp.weixin.qq.com/s/Jymq2ho3VQx7-o6y-JHxeg)
  - 提出几种延长语言模型输入长度的方法
- [赛尔笔记|大模型的涌现能力介绍](https://mp.weixin.qq.com/s/Jymq2ho3VQx7-o6y-JHxeg)
- [大模型也内卷，Vicuna训练及推理指南，效果碾压斯坦福羊驼](https://zhuanlan.zhihu.com/p/624012908)
- [在一张 24 GB 的消费级显卡上用 RLHF 微调 20B LLMs](https://mp.weixin.qq.com/s/jWBB2BQWHUqJk0OMo3ApXQ)
- [韩国庆熙大学等最新《生成式人工智能AIGC》综述](https://mp.weixin.qq.com/s/w185vd78lIKgpbXzNXiLOg)
- [GPT-4之高考评测](https://mp.weixin.qq.com/s/3NE1DAcIK42rbMRtPUMo8A)
- [ChatGPT 核心技术大起底——InstructGPT：研究人类反馈数据比加大模型规模更重要！](https://mp.weixin.qq.com/s/zzicRhuAbZ8zCOw8ok_S9w)
- [GPT解数学题准确率升至92.5%！微软提出MathPrompter，无需微调即可打造「理科」语言模型](https://mp.weixin.qq.com/s/BR7XDIjb0s07w9OInHrJLg)
- [总结开源可用的Instruct/Prompt Tuning数据](https://zhuanlan.zhihu.com/p/615277009)
- [总结当下可用的大模型LLMs](https://zhuanlan.zhihu.com/p/611403556)
- [RLHF魔法的衍生研究方向](https://mp.weixin.qq.com/s/535LhCV9GJPJGS3Jb5zbew)
- [ChatGPT应用端的Prompt解析：从概念、基本构成、常见任务、构造策略到开源工具与数据集](https://mp.weixin.qq.com/s/QJhqN6tn1FffDKdzWHuCAw)
- [ChatGPT等GPT-3.5系列大模型的鲁棒性如何？](https://mp.weixin.qq.com/s/LGf3Y_k8IokG1rqiX_4YpQ)
- [斯坦福等学者对ChatGPT做了在NLP几乎所有任务上的优劣势分析](https://mp.weixin.qq.com/s/xH89ENEMW6fWRoApLvgRZg)
- [ChatGPT 背后的“功臣”——RLHF 技术详解](https://mp.weixin.qq.com/s/TLQ3TdrB5gLb697AFmjEYQ)


## LLM和知识图谱
- [万字长文讲述大模型与知识图谱的关系](https://zhuanlan.zhihu.com/p/626433991)
- [LLM 时代的金融知识图谱实践](https://zhuanlan.zhihu.com/p/623104680)
  - 比较基础

