<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [汇总](#%E6%B1%87%E6%80%BB)
- [Transformer](#transformer)
- [tokenizer](#tokenizer)
- [BERT](#bert)
- [中文大模型](#%E4%B8%AD%E6%96%87%E5%A4%A7%E6%A8%A1%E5%9E%8B)
- [BERT fimaly](#bert-fimaly)
- [长序列预训练模型](#%E9%95%BF%E5%BA%8F%E5%88%97%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B)
- [Parameter-efficient](#parameter-efficient)
- [BERT前预训练](#bert%E5%89%8D%E9%A2%84%E8%AE%AD%E7%BB%83)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->



# 汇总

- [最新 Transformer 预训练模型综述！](https://mp.weixin.qq.com/s?__biz=MzI4ODY2NjYzMQ==&mid=2247495719&idx=1&sn=512c1f8b4e21c9d0117f625514a88290&chksm=ec38554fdb4fdc59470b34aab6a42822e75b045f45631f17339e81f1d50dbeba5bf02d991c33&mpshare=1&scene=24&srcid=1025ms9hMfaTN0AOZ2RkLPT1&sharer_sharetime=1635125386386&sharer_shareid=9d627645afe156ff11b0a8519d982bcd&exportkey=A62Gq4qJqdl1jfl%2BUsMVxvA%3D&pass_ticket=SHGOUtseKTQDhBbQUkxPd534tLY%2B6lmiRxoDIEirNdgCF3uij%2FoHBbS1BpQARUsW&wx_header=0#rd)
- [12个NLP预训练模型的学习笔记](https://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&mid=2650412585&idx=2&sn=0c1dbb5578e42c95a2da58586a8df357&chksm=becd907389ba1965a54a79d30146d76cbdd7ad6ae22acf998a7fea842a2babc8952683c768e7&scene=0&xtrack=1&exportkey=A6fw2cQ7DInpJlNoaWL0r40%3D&pass_ticket=peaJqRABUyiyXUkxShtHPoJ7onMoJTA4OFYeMuNaXmdNKq47G0x8XJEm7afGdVcX#rd)
- [融合检索和生成的SimBERT模型](https://spaces.ac.cn/archives/7427)
- 

# Transformer

- [浅谈Transformer模型中的位置表示](https://mp.weixin.qq.com/s/vXYJKF9AViKnd0tbuhMWgQ)
- [详解自注意力机制中的位置编码（第一部分）](https://zhuanlan.zhihu.com/p/352233973)
- [详解自注意力机制中的位置编码（第二部分）](https://zhuanlan.zhihu.com/p/354963727)
- [如何优雅地编码文本中的位置信息？三种positioanl encoding方法简述](https://mp.weixin.qq.com/s/ENpXBYQ4hfdTLSXBIoF00Q)

- [transformer中为什么使用不同的K 和 Q， 为什么不能使用同一个值？](https://www.zhihu.com/question/319339652)
- [transformer面试题的简单回答](https://zhuanlan.zhihu.com/p/363466672)

- [在注意力中重新思考Softmax：分解非线性，这个线性Transformer变体实现多项SOTA](https://mp.weixin.qq.com/s/tQmRUee_Yj-J_ljZGV1uFQ)
- [FLASH：可能是近来最有意思的高效Transformer设计](https://kexue.fm/archives/8934)
- [Transformer的一家！](https://mp.weixin.qq.com/s/f1OajymdtvswzFw_UunLlw)
- [分解非线性，这个线性transformer变体实现多项SOTA](https://mp.weixin.qq.com/s/7CRFr0fEQx7mpgWtAzAeXA)
- 

# tokenizer
- [BPE 算法详解](https://wmathor.com/index.php/archives/1517/)
- [BERT 是如何分词的](https://zhuanlan.zhihu.com/p/132361501)
- sentencePiece
  - [NLP-sentencepiece](https://zhpmatrix.github.io/2019/04/26/sentencepiece/)
  - [自然语言处理之_SentencePiece分词](https://blog.csdn.net/xieyan0811/article/details/80463147)
  - [中英文的whole word masking以及sentencepiece](https://zhuanlan.zhihu.com/p/366396747)


# BERT

- [GELU 激活函数](https://blog.csdn.net/liruihongbob/article/details/86510622)
- [BERT类模型的特殊符号的使用可以有哪些调整和变体？可以起到怎样的效果？](https://www.zhihu.com/question/387534279)
- [finetuning Bert时的权重衰减](https://blog.csdn.net/mch2869253130/article/details/105994044)
- [transformer中的positional encoding(位置编码)](https://blog.csdn.net/Flying_sfeng/article/details/100996524)
- [Bert/Transformer模型的参数大小计算](https://blog.csdn.net/weixin_43922901/article/details/102602557)

# 中文大模型

- [悟道大模型](https://wudaoai.cn/home)
- [“封神榜”大模型](https://www.idea.edu.cn/fengshenbang-lm.html)
  - https://github.com/IDEA-CCNL/Fengshenbang-LM


# BERT fimaly

- [StructBERT(ALICE) 详解](https://zhuanlan.zhihu.com/p/103207343)
- [万字长文带你纵览 BERT 家族](https://mp.weixin.qq.com/s?__biz=MzI0ODQ0MzM0Nw==&mid=2247483918&idx=1&sn=ddf308d199a3dfca210c3e581e81a031&chksm=e9a1fab2ded673a4973623227e65bcbbc38c0c216185e69b9becaa7c5e9ef2abffda498c958b&mpshare=1&scene=24&srcid=&sharer_sharetime=1590369637213&sharer_shareid=9d627645afe156ff11b0a8519d982bcd&exportkey=A4u14NHMiTik9sAsx%2BaPnQc%3D&pass_ticket=LlL6Ad5uohnLAlqJrzan%2BA5dDM3m9%2Bnl4L%2FaTWpnfTNnifRhbExGygOrgXBzVB7b&wx_header=0#rd)
- [MASS: 用于语言生成的遮蔽序列到序列与训练模型](https://zhuanlan.zhihu.com/p/67891175)
- [定向写作模型CTRL](https://zhuanlan.zhihu.com/p/100845592)
- [T5 模型：NLP Text-to-Text 预训练模型超大规模探索](https://blog.csdn.net/hecongqing/article/details/103404315)
- [从语言模型到Seq2Seq：Transformer如戏，全靠Mask](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw%3D%3D&chksm=96ea1f91a19d9687af5dbe751accc9c1ddd7392f6cf4294dc8a024751a64053cbcb8c60ef8f8&idx=1&mid=2247499793&scene=21&sn=685c54d27186a89dcf32d91ce0927274#wechat_redirect)
- [层次分解位置编码，让BERT可以处理超长文本](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw%3D%3D&chksm=96ea6235a19deb23babde5eaac484d69e4c2f53bab72d2e350f75bed18323eea3cf9be30615b&idx=1&mid=2247515573&scene=21&sn=2d719108244ada7db3a535a435631210#wechat_redirect)
- [词向量之GPT-1，GPT-2和GPT-3](https://zhuanlan.zhihu.com/p/350017443)
- [《Longformer: The Long-Document Transformer》论文笔记](https://zhuanlan.zhihu.com/p/134748587)
- [MacBERT 的改进（Revisiting Pre-Trained Models for Chinese Natural Language Processing）](https://blog.csdn.net/weixin_40122615/article/details/109317504)
- [BART原理简介与代码实战](https://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&mid=2247484720&idx=1&sn=e5109a1dddf87d0e59496389003018d8&chksm=97aee3e2a0d96af4e5f412a8caa21134ef82d48eb374c2bcc2b5b68ac62f65fe09b4793c6ef8&scene=0&xtrack=1&exportkey=A%2BV2SkDZneNY4W%2FS0tX%2FiPg%3D&pass_ticket=LlL6Ad5uohnLAlqJrzan%2BA5dDM3m9%2Bnl4L%2FaTWpnfTNnifRhbExGygOrgXBzVB7b&wx_header=0#rd)
- [ENRIE(Tsinghua)：知识图谱与BERT相结合，为语言模型赋能助力](https://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&mid=2650415030&idx=3&sn=86e28c9da6f90fb46a7d16aade947de0&chksm=becd99ec89ba10fadff1935e38ab25a665397ce37942b297c66a37889666dc56edbbeb133e06&mpshare=1&scene=24&srcid=07319ixYcn1arNxEeuAwLPYw&sharer_sharetime=1596189952608&sharer_shareid=9d627645afe156ff11b0a8519d982bcd&exportkey=Azal0BPq0SW8c0Q0eRaUuWU%3D&pass_ticket=IL%2BeHRprAt5yAlLjjC250jaLkeHDOYyDyV4vRbYX%2F0r7c3KJ%2FwPqrBhOiTesV9Z9&wx_header=0#rd)
- [XLNet:运行机制及和Bert的异同比较](https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247489553&idx=2&sn=68d679ea25168b81ad86e8ce857b5a19&chksm=ebb420c5dcc3a9d36a6031c6bffb3f7e33333061b5f3bf8d783a19e5438df4616f51b2b47312&scene=0&xtrack=1&pass_ticket=5l2GTJoNs3UnPjzRsDzXqTZBP6%2Btylp4BwIFxk3aFUwONC5l8MJz3gdjYHCbXS%2FH#rd)
- [ACL2021论文之ChineseBERT：融合字形与拼音信息的中文预训练模型](https://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&mid=2247484743&idx=1&sn=b715a22da6dac639b1c98f6da5e903d8&chksm=cfcad97ff8bd5069246102b4797fa55f8f9fa08291f15c5f1411560920d4f9f7fef680ac64a2&scene=21#wechat_redirect)
- [OPPO小布推出预训练大模型OBERT，晋升KgCLUE榜首](https://blog.csdn.net/m0_59407274/article/details/125601261)
- [ACL2022 | NoisyTune：微调前加入少量噪音可能会有意想不到的效果](https://mp.weixin.qq.com/s/6VXlc7GCjOM7BL4tPx9Mfw)

# 长序列预训练模型

- [Funnel-Transformer：让Transformer更高效地处理长序列](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247509847&idx=3&sn=d5f3c8306eb57dd652e9ba3c59f00524&chksm=96ea78d7a19df1c143a8cd0b2a528e8739c57e4b0d7728e5ba942d44871ca7ec6bc730b42228&mpshare=1&scene=24&srcid=0731BIn74GuIzLEZJspB4QbG&sharer_sharetime=1596187559415&sharer_shareid=9d627645afe156ff11b0a8519d982bcd&exportkey=A6eiuTHACMfX8ftAIHfv760%3D&pass_ticket=%2Fiuk0Yfg7CrYxacY%2F347pmZcCE1UxpnHXEwngLMc%2BDJTSlAVtev8q4cY8e9W%2Bxmv&wx_header=0#rd)
- [线性Attention的探索：Attention必须有个Softmax吗？](https://kexue.fm/archives/7546)
- [【论文阅读】Performer | Rethinking Attention With Performers](https://blog.csdn.net/yideqianfenzhiyi/article/details/112631729)
- [Transformer-XL解读（论文 + PyTorch源码）](https://blog.csdn.net/Magical_Bubble/article/details/89060213)
  - 在每个片段内计算self-attention
  - query向量的计算方式不变，key和valye向量在计算时，先在上层输出结果上拼接前一个片段的隐向量，然后乘以权重参数得到key和value向量，通过这种方式来引入上个片段的信息。
- [XLNet原理](http://fancyerii.github.io/2019/06/30/xlnet-theory/)
- [NEZHA（哪吒）论文阅读笔记](https://zhuanlan.zhihu.com/p/100044919)
- [层次分解位置编码，让BERT可以处理超长文本](https://kexue.fm/archives/7947)
- [Big Bird：支持更长序列的 Transformer](https://baijiahao.baidu.com/s?id=1674061802676524773&wfr=spider&for=pc)
- [Longformer：超越RoBERTa，为长文档而生的预训练模型](https://blog.csdn.net/xixiaoyaoww/article/details/107398795)
- [对Reformer的深入解读](https://zhuanlan.zhihu.com/p/115741192)


# Parameter-efficient
- [Parameter-Efficient Fine-tuning 相关工作梳理](https://mp.weixin.qq.com/s/sOPxL_Lq4lg3tbIsmEoMuw)
  - 综述性
- [Parameter-efficient transfer learning系列之Adapter](https://mp.weixin.qq.com/s/GHF2T0fk7PpolX-zqbDJJQ)
- [Parameter-efficient transfer learning系列之LoRA与BitFit](https://mp.weixin.qq.com/s/ge7jPUAGK4KSQ4dAlGLnrA)


# BERT前预训练

- [ELMo模型解读](https://blog.csdn.net/firesolider/article/details/88092831)
- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)
- [NLP之——Word2Vec详解](https://www.cnblogs.com/guoyaohua/p/9240336.html)
- [理解GloVe模型（+总结）](https://blog.csdn.net/u014665013/article/details/79642083)