[TOC]



## Paper

- [别让AI闭门造车！领域专业知识注入实战指南](https://mp.weixin.qq.com/s/AZb6mEQe4E_v4qeD8ExW7A)
- [如何高效地注入知识？ 大型语言模型预训练的知识注入缩放律](https://mp.weixin.qq.com/s/33Hk6CmDEXRFpx_CaFdYQg)
- [谁说SFT不能注入知识？](https://mp.weixin.qq.com/s/TLsHeVUl3ahr86qsOgfR9Q)
- [大模型知识注入微调](https://zhuanlan.zhihu.com/p/700753828)
- [大模型训练技术总结2-知识注入分析](https://zhuanlan.zhihu.com/p/1932851943493067040)
- Generation-Augmented Generation: A Plug-and-Play Framework for Private Knowledge Injection in Large Language Models
  - 2026.01
  - 作者认为主流两条路各有硬伤：Fine-tuning：迭代成本高、持续更新容易灾难性遗忘，并可能损伤通用能力。RAG：不改底座但在私有语料里容易脆——chunk 导致证据碎片化、检索漂移、长上下文压力让 prompt 随查询膨胀且不稳定。
  - GAG（Generation-Augmented Generation）把私有知识当作一种专家模态，用 1 个连续 token 注入，像多模态对齐那样，走表征级接口。对每个私有域，系统有一个轻量域专家模型，它会先基于问题生成一段专家背景文本，取其隐藏层表征，压缩成一个向量，接着用一个小投影器，把该向量映射到底座模型输入 embedding 空间，最后在底座LLM增加 1 个连续 token。
  - 第二步：两阶段训练，把领域能力和对齐接口分开学，作者把训练拆成两件事：先让专家变强，再让接口对齐到底座。用领域的 QA以自回归目标训练领域专家。然后只训练投影器，让注入 token 在底座LLM空间可用。
  - 支持多领域部署：提出 Prototype Plug-and-Play Routing (PPR)，用冻结编码器把历史 query embed 后，每个领域用 KMeans 聚成原型集合，对新 query，计算与各域原型的最大余弦相似度，取最大者作为路由结果。
  - 论文中底座用冻结 Qwen3-8B，域专家用 Qwen3-1.7B；投影器是两层 MLP+GELU；路由用 Qwen3-1.7B 做冻结 query encoder。
  - GAG私有域提升大，而且只加 1 token，通用能力基本不掉，路由几乎接近 oracle，支持增量加域。
- Continual Learning via Sparse Memory Finetuning
  - 2025.10
  - 研究背景：模型部署后如果继续用新数据更新，往往会灾难性遗忘。作者提出一个直觉：遗忘难缓解，很大原因是所有任务共享同一套可训练参数，新知识更新会和旧知识抢参数、互相干扰。
  - 作者基于记忆层（memory layer）模型来做持续学习：把 Transformer 中间的一层 FFN 替换成一个可查询的大规模参数记忆池（keys/values），每个 token 只会激活 top-k（如 k=32）个记忆槽位，因此天然是稀疏访问。
  - 对一个训练 batch，统计这个 batch 访问了哪些记忆槽位，并不是访问多就该更新，作者用 TF-IDF 的思想做相对重要性，用 TF-IDF 评分把槽位排个序。
  - 每个梯度步只让 排名前 t 的槽位参与反向传播，其余槽位（以及模型其它部分）梯度被截断/冻结，从而降低新旧知识的参数干扰。
  - 论文主要对比三类更新方式：Full finetuning（全参微调）、LoRA（参数高效微调）、Sparse Memory Finetuning。本文提出的方法学得一样多，但忘得少很多。
- Pretraining with hierarchical memories: separating long-tail and common knowledge
  - 2025.10
  - 研究背景：把所有世界知识都压缩进大模型参数里既不必要，也不适合边缘设备（推理时显存/算力受限；而每个 prompt 实际只用到一小部分知识）。
  - 第一步：把预训练语料做成分层簇，让检索可扩展：他们对预训练数据做层次聚类：先用文本嵌入模型把每篇文档映射成向量，然后用层次 k-means 把数据分成簇 → 子簇 → 更细子簇…。
  - 第二步：推理时取一小块参数，而不是取一堆文本：与 RAG 取文本拼 prompt不同，这里取的是参数块，给定输入上下文，retriever 从巨大记忆库里取出与该上下文匹配的记忆参数块，把这块参数加到锚点模型里一起前向计算。
  - 第三步：选什么样的参数记忆最有效：FFN 记忆胜出。他们系统比较了几类把额外参数接入 Transformer的方式：LoRA 型记忆、KV 记忆（学习额外 KV 向量，让 query 去 cross-attend）、FFN-Memories（扩展 SwiGLU FFN 的内层维度）。
  - 第四步：训练策略：区分 fetched memory（按上下文取） vs generic memory（固定加），并支持共同训练，为了证明提升不是因为参数变多，他们做了关键对照实验。
  - 实验结果：
    - 1）最核心主结果：小模型 + 少量 fetched memory ≈ 更大密集模型
    - 2）长尾知识提升非常直观：元素原子序数任务的极端案例
    - 3）不仅能从头训，还能后装：对开源模型做 post-hoc memory learning
    - 4）与 RAG 的对比：记忆在 FLOPs/存储上更硬件友好，而且效果更稳
    - 5）一个很应用向的小实验：阻断记忆块会显著掉点（隐私/隔离潜力）
- Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning
  - 2024.04
  - 研究背景：大模型的知识被预训练语料与截止日期（knowledge cutoff）锁死，一旦要回答模型截止日期之后发生的新事实，就容易答错或胡编。作者希望探索：能不能不靠检索（RAG），而是用更直接的训练方式把新知识写进模型参数里。
  - 论文核心是用 SFT（监督微调）做知识注入，关键难点在于——训练集怎么构造，才能覆盖到文档里的新事实。作者选了 6 篇维基百科文章：5 篇是post-cutoff 的 2023 体育赛事，另 1 篇是 pre-cutoff 的 2018 世界杯（当作对照）。并通过 API 抽取各 section 的纯文本、过滤空段落/表格段落等。
  - 作者对同一批文档生成两类 Q&A 数据集，并做 1x/5x/10x 的规模扩展（scaling）实验，核心差别在你扩展的是 tokens 还是 facts。
    - Token-based scaling：对每个 section 先算 token 数，然后不断让 GPT-4 生成不同 Q&A，直到生成的 Q&A token 数超过原文 section token 数的 10 倍；再从中抽子集得到 1x/5x/10x。它们不一定均匀覆盖所有知识点；很容易出现某些容易问的点被反复问、一些冷门点完全没人问
    - Fact-based scaling：先让 GPT-4 把文档拆成一条条 atomic facts（原子事实），然后对每条事实生成多组（例如 10 组）不同问法的 Q&A，保证每个事实都有训练样本覆盖；同样再抽子集得到 1x/5x/10x。每个事实都有样本，扩展时是在增加同一事实的多样问法/表达，所以覆盖更系统、更均匀。
  - 他们微调的是 GPT-4 v0613（文中提到其截止日期为 2021 年 9 月）
  - 实验结果，Token-based：能涨分，但覆盖不均、后期收益变差甚至退化；Fact-based：扩展更稳定，没有规模越大反而退化的问题