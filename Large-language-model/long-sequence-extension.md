


# Paper

### 2023

- Alibi
  - [Alibi位置向量外推性：看起来很长其实还是短](https://developer.aliyun.com/article/842370)
  - [ALiBi - 给注意力加上线性偏置](https://zhuanlan.zhihu.com/p/632780188)
  - https://www.mosaicml.com/blog/mpt-7b 在相对短的文本上预训练，然后在长文本上微调

- https://kaiokendev.github.io/til#extending-context-to-8k
  - 基于RoPE，scaling down frequency window


# Blog

- [NTK-ALiBi：通过插值实现大模型ALiBi位置编码的长文本外推](https://zhuanlan.zhihu.com/p/647628295)

- [浅谈LLM的长度外推](https://zhuanlan.zhihu.com/p/645770522)

- [LLM长度外推研究1——外推结果及原因分析](https://blog.csdn.net/maxsen_jn/article/details/132517811)

- [Transformer升级之路：7、长度外推性与局部注意力](https://spaces.ac.cn/archives/9431)

- https://kaiokendev.github.io/context
  - 综述性blog

- [聊聊拉长LLaMA的一些经验](https://zhuanlan.zhihu.com/p/647145964)

- [大模型位置编码及其外推性](https://mp.weixin.qq.com/s/OGP49dzhXfIudHEGHOVPcw)