[TOC]



## 线程块、线程网格和索引的计算

CUDA 中的线程块（block）和线程网格（grid）是多维的。默认情况下，`blockIdx` 和 `threadIdx` 都是三维的，分别包含 x、y、z 维度的索引。所以，在 CUDA 中每个线程块（block）可以有多个维度（如：`blockIdx.x`, `blockIdx.y`, `blockIdx.z`），每个线程也可以在多个维度上进行索引（如：`threadIdx.x`, `threadIdx.y`, `threadIdx.z`）。

然而，实际上你不一定需要使用所有的维度。如果你的问题是二维或一维的，你可以只使用 `x` 维度，省略 `y` 和 `z` 维度。**因此具体使用哪些维度，主要是根据所要计算的数据决定的，当然与计算方法也有一定关系。**

具体来说：

- **blockIdx**: 用于表示线程块在网格中的位置。`blockIdx.x` 表示块在 x 维度上的位置，`blockIdx.y` 和 `blockIdx.z` 分别表示 y 和 z 维度上的位置。
- **threadIdx**: 用于表示线程在当前线程块中的位置。`threadIdx.x` 表示线程在 x 维度上的位置，`threadIdx.y` 和 `threadIdx.z` 分别表示 y 和 z 维度上的位置。



**一维列表计算例子**

```c++
__global__ void vecAdd(const double *x, const double *y, double *z, const int N)
{
    // 计算当前线程在一维数据中的全局索引
    const int index = blockDim.x * blockIdx.x + threadIdx.x;

    // 判断索引是否超出数组的边界
    if (index < N)
    {
        // 进行数组元素的加法
        z[index] = x[index] + y[index];
    }
}

```

有似于对数据进行分段。



**二维网格计算例子**

```c++
__global__ void vecAdd(const double *x, const double *y, double *z, const int N)
{
    const int index_x = blockDim.x * blockIdx.x + threadIdx.x;
    const int index_y = blockDim.y * blockIdx.y + threadIdx.y;
    const int index = index_y * gridDim.x * blockDim.x + index_x;
    if (index < N)
    {
        z[index] = x[index] + y[index];
    }
}

```

类似对二维网格，划分成多个小的二维网格。

其中`gridDim.x * blockDim.x`其实就是`x`方向的行的数量，比如对于一个`M*N`(x方向, y方向)的网格，那么`gridDim.x * blockDim.x`其实就是M。



**通用的全局索引计算**

```c++
__global__ void print_thread_idx_per_grid_kernel()
{
	// 一个block中含有的线程数量
    int bSize = blockDim.z * blockDim.y * blockDim.x;

	// block的全局index号
    int bIndex = blockIdx.z * gridDim.x * gridDim.y +
                 blockIdx.y * gridDim.x +
                 blockIdx.x;
	
	// block内的线程索引
    int tIndex = threadIdx.z * blockDim.x * blockDim.y +
                 threadIdx.y * blockDim.x +
                 threadIdx.x;
	
	// 全局唯一索引
    int index = bIndex * bSize + tIndex;

    printf("block idx: %3d, thread idx in block: %3d, thread idx: %3d\n",
           bIndex, tIndex, index);
}


```

## cudaMalloc的参数解释

```c++
// 调用 cudaMalloc 分配内存
cudaMalloc((void **)&d_z, M);
```

cudaMalloc 函数的第一个参数：(void \*\*)&d_z，cudaMalloc 函数需要一个类型为 void \*\* 的参数。这个参数的作用是传递一个指针的地址，以便 cudaMalloc 在执行时能够修改它，让它指向分配的设备内存。

cudaMalloc 函数会修改传入的指针，使它指向 GPU 上分配的内存。

当调用 cudaMalloc 时，你给它传入的是 &d_z（即 double ** 类型），所以 cudaMalloc 会将 d_z 指向 GPU 上分配的内存块。

cudaMalloc 会在内部修改 d_z 的值，使得它指向实际的设备内存。为了能让 d_z 在函数外部也能反映这个修改，必须通过 &d_z 传递它的地址，也就是传递指向指针的指针（void **）。

## CUDA核心与线程的关系

**GPU线程的实际含义：**

- GPU线程（Thread）在GPU编程模型（例如CUDA或OpenCL）中，是一个逻辑上的概念。
- GPU同时管理数以万计的线程，但并不是所有线程同时运行，而是分批调度执行。

**CUDA核心的实际含义：**

- CUDA核心本质上是GPU中负责执行**单个简单指令**（比如一次浮点运算）的基本执行单元，类似于CPU的算术逻辑单元(ALU)。
- 一个CUDA核心每个时钟周期只能执行一个简单操作（例如一次浮点加法或乘法）。

**实际执行情况：**

- GPU线程以**Warp**（在NVIDIA中，Warp通常包含32个线程）为单位调度。一个Warp内的32个线程会同步执行同一条指令（SIMT模型）。
- 当GPU执行一个Warp（32个线程）时，如果这个Warp调度到Streaming Multiprocessor (SM) 上，而SM中有足够的CUDA核心（例如32个CUDA核心），则该Warp的全部32个线程会同时并行执行，每个CUDA核心执行一个线程的同一条指令。
- 如果Warp大小超过CUDA核心的数量（或者可用核心不足），那么GPU则需要多次执行（多个时钟周期）才能完成Warp中所有线程的运算。

**关键的理解：**

- CUDA线程是逻辑抽象的概念，用于管理并行性和任务。
- CUDA核心是物理单元，是线程任务在硬件上实际执行的地方。
- CUDA核心可以在不同时间点执行不同的线程指令，因此**线程数远远大于CUDA核心数**是很正常的情况（GPU往往同时管理上万个线程，但物理CUDA核心数量却只有几百或几千个）。

## 常见CUDA算法

### Scan算法（前缀和）

下面的算法均是只在block内进行前缀和的计算。

1. 使用双缓冲区

   ```c
   template <int threadsPerBlock, int numElements>
   __global__ void kernel_2(int *input, int *output) {
     __shared__ int _buffer_one[threadsPerBlock];
     __shared__ int _buffer_two[threadsPerBlock];
   
     const int tid = threadIdx.x;
     const int gtid = blockIdx.x * threadsPerBlock + tid;
   
     int *buffer_one = _buffer_one;
     int *buffer_two = _buffer_two;
   
     buffer_one[tid] = input[gtid];
     __syncthreads();
   
   #pragma unroll
     for (unsigned int offset = 1; offset <= threadsPerBlock / 2; offset <<= 1) {
       if (tid >= offset) {
         buffer_two[tid] = buffer_one[tid] + buffer_one[tid - offset];
       } else {
         buffer_two[tid] = buffer_one[tid];
       }
       __syncthreads();
   
       int *tmp = buffer_one;
       buffer_one = buffer_two;
       buffer_two = tmp;
     }
   
     if (gtid < numElements) {
       output[gtid] = buffer_one[tid];
     }
   }
   
   template <int threadsPerBlock, int numElements>
   void launch_kernel_2(int *input, int *output) {
     const int numBlocks = (numElements + threadsPerBlock - 1) / threadsPerBlock;
     kernel_2<threadsPerBlock, numElements>
         <<<numBlocks, threadsPerBlock>>>(input, output);
   }
   ```

   

2. 使用warp原语

   ```c
   #define WARP_SIZE 32
   #define LOG_WARP_SIZE 5
   #define WARP_MASK (WARP_SIZE - 1)
   
   // 将块中的线程id映射到warp中的线程id
   __device__ inline int lane_id(void) { return threadIdx.x & WARP_MASK; }
   // 计算warp的index
   __device__ inline int warp_id(void) { return threadIdx.x >> LOG_WARP_SIZE; }
   
   // Warp scan：在一个warp内计算前缀和
   // 为何不需要线程同步？__shfl_up_sync自动同步语义
   __device__ __forceinline__ int warp_scan(int val) {
     int x = val;
   #pragma unroll
     for (int offset = 1; offset < WARP_SIZE; offset <<= 1) {
       int y = __shfl_up_sync(0xffffffff, x, offset);
       if (lane_id() >= offset) x += y;
     }
     return x - val;
   }
   
   template <int threadsPerBlock>
   __device__ int block_scan(int in) {
     // sdata保存每个warp经过scan之后的 warp内最后一个线程的值
     // threadsPerBlock一般不会大于1024，因此sdata长度不会大于32，因此可以使用一个warp进行scan
     __shared__ int sdata[threadsPerBlock >> LOG_WARP_SIZE];
     // A. Exclusive scan within each warp
     int warpPrefix = warp_scan(in);
     // B. Store in shared memory
     if (lane_id() == WARP_SIZE - 1) sdata[warp_id()] = warpPrefix + in;
     __syncthreads();
     // C. One warp scans in shared memory
     if (threadIdx.x < WARP_SIZE)  // 可以使用一个warp执行scan
       sdata[threadIdx.x] = warp_scan(sdata[threadIdx.x]);
     __syncthreads();
     // D. Each thread calculates its final value
     // warpPrefix是当前线程在warp内的前缀和（不包含当前线程对应的元素）+ 前面所有warp的前缀和 = 当前线程的前缀和（不包含当前线程对应的元素）
     int thread_out_element = warpPrefix + sdata[warp_id()];
     return thread_out_element;
   }
   
   template <int threadsPerBlock, int numElements>
   __global__ void kernel_3(int *input, int *output) {
     int gtid = threadIdx.x + blockIdx.x * blockDim.x;
     int val = input[gtid];
     int result = block_scan<threadsPerBlock>(val);
     if (gtid < numElements) {
       output[gtid] = result + val;
     }
   }
   
   // 主要该方法只进行block内前缀和计算
   template <int threadsPerBlock, int numElements>
   void launch_kernel_3(int *input, int *output) {
     const int numBlocks = (numElements + threadsPerBlock - 1) / threadsPerBlock;
     kernel_3<threadsPerBlock, numElements>
         <<<numBlocks, threadsPerBlock>>>(input, output);
   }
   ```


## CUDA流

### 传统默认流和每线程流

传统默认流与所有非默认阻塞流双向隐式同步，而每线程默认流与其他流无隐式同步，彼此独立。截至CUDA 11.4，默认的构建参数仍然是传统默认流`legacy`，必须手动将其更改为`per-thread`才能使用每线程默认流。

对于创建的非默认非阻塞流，即使是传统默认流，也无法与非默认非阻塞流同步。

### 基于CUDA流实现并发

串行模型：常规流程是先将输入数据从主机内存拷贝到设备内存，接着执行核函数进行计算以得到输出结果，最后再将输出结果从设备内存拷贝回主机内存。

并发模型：将数据进行拆分给不同的cuda流进行处理，不同cuda流之间互不依赖，从而使得内存拷贝、核函数执行可以并行。具体并发过程如下：

| 操作                         |               |                 |                 |                 |                 |
| ---------------------------- | ------------- | --------------- | --------------- | --------------- | --------------- |
| 主机到设备引擎（H2D Engine） | （内存拷贝）1 | （内存拷贝）2   | （内存拷贝）3   | （内存拷贝）4   | ——              |
| 核函数引擎                   | ——            | （核函数执行）1 | （核函数执行）2 | （核函数执行）3 | （核函数执行）4 |
| 设备到主机引擎               | ——            | ——              | （内存拷贝）1   | （内存拷贝）2   | （内存拷贝）3   |
| 时间（Time）                 | 时刻 1        | 时刻 2          | 时刻 3          | 时刻 4          | 时刻 5          |



除了内存拷贝和核函数执行之间的并行之后，核函数执行也可以并行。但是核函数的并行度受到计算资源的影响，比如，当block值设置较小时，那么平均每个kernel被分配的计算资源就越多，那么kernel并行度也就越高。由于block值设置的比较小，线程少，完成同样的数据规模的计算，kernel执行时间也就越长。

相反，如果block值设置的比较大，平均每个kernel被分配的计算资源越少，那么kernel并行度就越低。由于block值设置的比较大，线程多，完成同样数据规模的计算，kernel执行时间也就越短。

传统默认流由于存在隐式同步，即使每个kernel被分配足够多的计算资源，也会出现没有kernel并行，即主机线程向default Stream 发出的CUDA命令在来自其他不同流的其他CUDA命令之间造成隐式同步。解决方法是使用每线程默认流。

## 向量化拷贝

向量化拷贝的本质是用硬件友好的方式组织内存访问，通过 “批量处理 + 匹配事务粒度”，将原本低效的小尺寸数据拷贝转换为高效的大块内存操作，最终实现带宽利用率提升、指令开销降低、延迟减少的综合性能优势。

```cpp
#include <cstdint>
#include <cuda_runtime.h>

/**
 * @brief 8字节粒度向量化内存拷贝核函数（GPU端执行）
 * @tparam T 基础数据类型（如int8_t、int16_t、int32_t等）
 * @param output 设备端（GPU）输出缓冲区指针（__restrict__：无别名优化）
 * @param input 设备端（GPU）输入缓冲区指针（__restrict__：无别名优化）
 * @param n 需要拷贝的「T类型基础单元总数」（非字节数）
 */
template <typename T>
__global__ void vectorized_memcpy_8byte(
    T* __restrict__ output,    // 输出缓冲区（GPU端）
    const T* __restrict__ input,  // 输入缓冲区（GPU端）
    size_t n                   // 待拷贝的T类型单元总数
) {
    // 1. 计算当前线程的「全局索引」（整个GPU网格中唯一标识）
    // blockIdx.x：当前线程块在网格中的X维度索引
    // blockDim.x：每个线程块包含的线程数（X维度）
    // threadIdx.x：当前线程在线程块中的X维度索引
    size_t thread_global_idx = blockDim.x * blockIdx.x + threadIdx.x;

    // 2. 计算「网格步长」= 总线程数（所有线程块的线程总和）
    // 作用：当数据量超过线程总数时，线程通过步长跨步访问，避免重复处理
    size_t grid_stride = blockDim.x * gridDim.x;

    // 3. 编译期计算：1个8字节单元（uint64_t）包含多少个T类型基础单元
    // sizeof(uint64_t)：固定8字节；sizeof(T)：T类型的字节数（如int8_t是1字节）
    // 例：T=int8_t → 8/1=8（1个8字节单元含8个int8_t）；T=int16_t → 8/2=4
    constexpr size_t t_units_per_8byte = sizeof(uint64_t) / sizeof(T);

    // 4. 循环处理：按8字节单元粒度分配任务（线程跨步访问）
    // 循环变量i：8字节单元的索引（而非T类型单元索引）
    // 循环条件：i * t_units_per_8byte < n → 当前8字节单元覆盖的T单元未超出总数量
    // 步长grid_stride：每个线程处理完一个8字节单元后，跳过总线程数个单元
    for (size_t i = thread_global_idx; i * t_units_per_8byte < n; i += grid_stride) {
        // 5. 判断当前8字节单元是否「完全有效」（无越界）
        // (i+1)*t_units_per_8byte ≤ n → 下一个8字节单元的起始T索引未超界
        if ((i + 1) * t_units_per_8byte <= n) {
            // -------------------------- 核心：8字节向量化拷贝 --------------------------
            // ① 指针重解释：将T*转换为uint64_t*（告诉编译器按8字节粒度解读内存）
            // ② 单条赋值指令：一次性拷贝8字节数据（替代多次T单元拷贝）
            // 例：T=int8_t时，这行代码 = 8次output[j] = input[j]的效率
            reinterpret_cast<uint64_t*>(output)[i] =  // 输出缓冲区按8字节解读
            reinterpret_cast<const uint64_t*>(input)[i];  // 输入缓冲区按8字节解读
            // --------------------------------------------------------------------------
        } else {
            // 6. 边界处理：最后一个8字节单元部分有效（不足8字节，避免越界）
            // 计算剩余T单元的「起始索引」（当前8字节单元对应的第一个T单元）
            size_t remaining_t_start = i * t_units_per_8byte;
            // 计算剩余T单元的「数量」（总T数 - 起始索引）
            size_t remaining_t_count = n - remaining_t_start;

            // 逐个拷贝剩余的T单元（退化到单单元拷贝，确保不越界）
            for (size_t j = 0; j < remaining_t_count; j++) {
                output[remaining_t_start + j] = input[remaining_t_start + j];
            }
        }
    }
}


/**
 * @brief 8字节向量化拷贝的启动函数（CPU端调用，配置并启动核函数）
 * @tparam T 基础数据类型（与核函数T一致）
 * @param output 设备端输出缓冲区指针（需提前用cudaMalloc分配）
 * @param input 设备端输入缓冲区指针（需提前用cudaMalloc分配）
 * @param n 待拷贝的T类型单元总数
 * @param stream 用于异步执行的CUDA流（nullptr表示默认流）
 */
template <typename T>
void launch_vectorized_memcpy_8byte(
    T* output,
    const T* input,
    size_t n,
    cudaStream_t stream = nullptr
) {
    // 配置线程块大小：每个线程块固定1024个线程（CUDA硬件最优值，支持大多数GPU）
    dim3 threads_per_block(1024);  // 仅用X维度（1D线程块）

    // 计算8字节单元的总数量（向上取整，避免遗漏最后一个部分有效单元）
    // 公式：(总T字节数 + 8字节 - 1) / 8字节 → 向上取整
    size_t total_8byte_units = (n * sizeof(T) + sizeof(uint64_t) - 1) / sizeof(uint64_t);

    // 计算线程块总数（向上取整，确保覆盖所有8字节单元）
    // 限制：块数不超过unsigned int最大值（避免超出GPU硬件限制）
    dim3 blocks_per_grid(
        static_cast<unsigned int>(
            std::min(
                (total_8byte_units + threads_per_block.x - 1) / threads_per_block.x,
                static_cast<size_t>(std::numeric_limits<unsigned int>::max())
            )
        )
    );

    // 启动核函数（<<<网格配置, 线程块配置, 共享内存大小, 流>>>）
    vectorized_memcpy_8byte<<<blocks_per_grid, threads_per_block, 0, stream>>>(
        output, input, n
    );

    // 检查核函数启动错误（CUDA核函数启动是异步的，需主动检查错误）
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        fprintf(stderr, "核函数启动失败：%s（文件：%s，行号：%d）\n", 
                cudaGetErrorString(err), __FILE__, __LINE__);
        exit(EXIT_FAILURE);
    }
}
```

