[TOC]



# Repo

- https://github.com/Unakar/Logic-RL
- https://github.com/eddycmu/demystify-long-cot
- https://github.com/RLHFlow/Minimal-RL
- https://github.com/cmu-l3/l1
- https://github.com/shangshang-wang/Tina
- https://github.com/hkust-nlp/simpleRL-reason
- https://github.com/sail-sg/understand-r1-zero
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero
- https://github.com/agentica-project/rllm
- https://github.com/lzhxmu/CPPO
- https://github.com/OpenManus/OpenManus-RL
  - 基于强化学习优化大模型智能体（类似R1算法）
- https://github.com/huggingface/open-r1
- https://github.com/aburkov/theLMbook/blob/main/GRPO_From_Scratch_Multi_GPU_DataParallel_Qwen_2_5_1_5B_Instruct.ipynb
- https://github.com/dhcode-cpp/X-R1

- Blog
  - [SPO新范式助力大模型推理能力提升](https://mp.weixin.qq.com/s/abUS_x8GTHEqTwS9rCwgLg)
    - 为连续token片段计算reward
  - [探讨Entropy(熵)机制在RL中扮演的角色](https://mp.weixin.qq.com/s/8eTzjE677C2jvyYtsEFukg)
  - [R1 的一些认知：4 个经典误区](https://mp.weixin.qq.com/s/FnNRLXxBCFxFy-1v8QigLQ)
  - [从Math RL初窥LLM推理模型：是怎么work、哪些trick是有效的！](https://mp.weixin.qq.com/s/5zlujCaxGY1CL8IZeGqT2g)
  - [[Experiment] Training R1-Zero-like models with Open R1](https://huggingface.co/spaces/open-r1/README/discussions/20)
  - [Reasoning 模型 RL 对齐的实际挑战](https://zhuanlan.zhihu.com/p/1892270905683575985)
  - [复现和改进 DeepSeek-R1 的一些 tips](https://mp.weixin.qq.com/s/xqWYdf2c9frWbznKNGiagA)
    - 2025.03

  - [浅谈 DeepSeek-R1 和 Kimi k1.5 论文中的思维链 + 强化学习](https://weaxsey.org/articels/2025-02-01/)
    - 2025.02
- Evaluation
  - https://github.com/QwenLM/Qwen2.5-Math

